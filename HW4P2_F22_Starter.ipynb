{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8a_cZqr-UpV"
      },
      "source": [
        "# HW4P2: Attention-based Speech Recognition\n",
        "\n",
        "Welcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with attention. <br> <br>\n",
        "\n",
        "HW Writeup: https://piazza.com/class_profile/get_resource/l37uyxe87cq5xn/lam1lcjjj0314e <br>\n",
        "Kaggle competition link: https://www.kaggle.com/competitions/11-785-f22-hw4p2/ <br>\n",
        "LAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\n",
        "Attention is all you need:https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlev_Tvq_bRz"
      },
      "source": [
        "# Initial Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0pueIzbxUwyY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Dec  6 09:55:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:0A:00.0  On |                  N/A |\n",
            "|  0%   41C    P8    25W / 215W |    587MiB /  8192MiB |      9%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1560      G   /usr/lib/xorg/Xorg                301MiB |\n",
            "|    0   N/A  N/A      1727      G   /usr/bin/gnome-shell               64MiB |\n",
            "|    0   N/A  N/A      3796      G   ...4/usr/lib/firefox/firefox      219MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr4xGzRU-KZz"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZectxKF3XEVV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import Levenshtein\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "from torchsummaryX import summary\n",
        "import wandb\n",
        "from glob import glob\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", DEVICE)\n",
        "\n",
        "from hparams import Hparams\n",
        "\n",
        "hparams = Hparams()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OALQCI0EDCwh"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AqDuibMCP345"
      },
      "outputs": [],
      "source": [
        "# Global config dict. Feel free to add or change if you want.\n",
        "config = {\n",
        "    'batch_size': 96,\n",
        "    'epochs': 30,\n",
        "    'lr': 1e-3\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7J4sY1OW9Pr"
      },
      "source": [
        "# Toy Data Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTsQB-pvRLLs"
      },
      "source": [
        "The toy dataset is very essential for you in this HW. The model which you will be building is complicated and you first need to make sure that it runs on the toy dataset. <br>\n",
        "In other words, you need convergence - the attention diagonal. Take a look at the write-up for this. <br>\n",
        "We have given you the following code to download the toy data and load it. You can use it the way it is. But be careful, the transcripts are different from the original data from kaggle. The toy dataset has phonemes but the actual data has characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Des4AMIaW4E8"
      },
      "outputs": [],
      "source": [
        "# !wget -q https://cmu.box.com/shared/static/wok08c2z2dp4clufhy79c5ee6jx3pyj9 --content-disposition --show-progress\n",
        "# !wget -q https://cmu.box.com/shared/static/zctr6mvh7npfn01forli8n45duhp2g85 --content-disposition --show-progress\n",
        "# !wget -q https://cmu.box.com/shared/static/m2oaek69145ljeu6srtbbb7k0ip6yfup --content-disposition --show-progress\n",
        "# !wget -q https://cmu.box.com/shared/static/owrjy0tqra3v7zq2ru7mocy2djskydy9 --content-disposition --show-progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2fLveDeiXCsb"
      },
      "outputs": [],
      "source": [
        "# Load the toy dataset\n",
        "X_train = np.load(\"f0176_mfccs_train.npy\")\n",
        "X_valid = np.load(\"f0176_mfccs_dev.npy\")\n",
        "Y_train = np.load(\"f0176_hw3p2_train.npy\")\n",
        "Y_valid = np.load(\"f0176_hw3p2_dev.npy\")\n",
        "\n",
        "# This is how you actually need to find out the different trancripts in a dataset. \n",
        "# Can you think whats going on here? Why are we using a np.unique?\n",
        "VOCAB_MAP           = dict(zip(np.unique(Y_valid), range(len(np.unique(Y_valid))))) \n",
        "VOCAB_MAP[\"[PAD]\"]  = len(VOCAB_MAP)\n",
        "VOCAB               = list(VOCAB_MAP.keys())\n",
        "\n",
        "SOS_TOKEN = VOCAB_MAP[\"[SOS]\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"[EOS]\"]\n",
        "PAD_TOKEN = VOCAB_MAP[\"[PAD]\"]\n",
        "\n",
        "Y_train = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_train]\n",
        "Y_valid = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_valid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iNjjsFqpbocR"
      },
      "outputs": [],
      "source": [
        "# Dataset class for the Toy dataset\n",
        "class ToyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, partition):\n",
        "\n",
        "        if partition == \"train\":\n",
        "            self.mfccs = X_train[:, :, :15]\n",
        "            self.transcripts = Y_train\n",
        "\n",
        "        elif partition == \"valid\":\n",
        "            self.mfccs = X_valid[:, :, :15]\n",
        "            self.transcripts = Y_valid\n",
        "\n",
        "        assert len(self.mfccs) == len(self.transcripts)\n",
        "\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        x = torch.tensor(self.mfccs[i])\n",
        "        y = torch.tensor(self.transcripts[i])\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        x_batch, y_batch = list(zip(*batch))\n",
        "\n",
        "        x_lens      = [x.shape[0] for x in x_batch] \n",
        "        y_lens      = [y.shape[0] for y in y_batch] \n",
        "\n",
        "        x_batch_pad = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value= EOS_TOKEN)\n",
        "        y_batch_pad = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value= EOS_TOKEN) \n",
        "        \n",
        "        return x_batch_pad, y_batch_pad, torch.tensor(x_lens), torch.tensor(y_lens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWRjucnUdbQ1"
      },
      "source": [
        "# Kaggle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fStTuAQ6XAuD"
      },
      "outputs": [],
      "source": [
        "# TODO: Use the same Kaggle code from HW1P2, HW2P2, HW3P2\n",
        "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "# !mkdir /root/.kaggle/\n",
        "\n",
        "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "#     f.write('') # Put your kaggle username & key here\n",
        "\n",
        "# !chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nR74ooCSa664"
      },
      "outputs": [],
      "source": [
        "# Download the data\n",
        "# !kaggle competitions download -c 11-785-f22-hw4p2\n",
        "# !mkdir '/content/data'\n",
        "\n",
        "# !unzip -qo '11-785-f22-hw4p2.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ioyn6ldQB9"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YHix8UvpBWh7"
      },
      "outputs": [],
      "source": [
        "# These are the various characters in the transcripts of the datasetW\n",
        "VOCAB = ['<sos>',   \n",
        "         'A',   'B',    'C',    'D',    \n",
        "         'E',   'F',    'G',    'H',    \n",
        "         'I',   'J',    'K',    'L',       \n",
        "         'M',   'N',    'O',    'P',    \n",
        "         'Q',   'R',    'S',    'T', \n",
        "         'U',   'V',    'W',    'X', \n",
        "         'Y',   'Z',    \"'\",    ' ', \n",
        "         '<eos>']\n",
        "\n",
        "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
        "\n",
        "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a dataset class which is exactly the same as HW3P2. You are free to reuse it.\n",
        "# The only change is that the transcript mapping is different for this HW.\n",
        "# Note: We also want to retain SOS and EOS tokens in the transcript this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zannKblcPORG"
      },
      "outputs": [],
      "source": [
        "# TODO: Similarly, create a test dataset class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQenneVsDLnX"
      },
      "source": [
        "# Dataset and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oi2FS_hkDQZB"
      },
      "outputs": [],
      "source": [
        "# TODO: Create the datasets and dataloaders\n",
        "# All these things are similar to HW3P2\n",
        "# You can reuse the same code\n",
        "\n",
        "# The sanity check for shapes also are similar\n",
        "# Please remember that the only change in the dataset for this HW is the transcripts\n",
        "# So you are expected to get similar shapes like HW3P2 (Pad, pack and Oh my!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--VjKlEhwi8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uql9E6cqROvJ"
      },
      "source": [
        "In this section you will be building the LAS model from scratch. Before starting to code, please read the writeup, paper and understand the following parts completely.<br>\n",
        "- Pyramidal Bi-LSTM \n",
        "- Listener\n",
        "- Attention\n",
        "- Speller\n",
        "\n",
        "After getting a good grasp of the workings of these modules, start coding. Follow the TODOs carefully. We will also be adding some extra features to the attention mechanism like keys and values which are not originally present in LAS. So we will be creating a hybrid network based on LAS and Attention is All You Need.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCbwz0LZMWwe"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoI0zEoIMX5I"
      },
      "source": [
        "### Pyramidal Bi-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rchbyjlMeB2"
      },
      "source": [
        "### Listener"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([96, 176, 15]) torch.Size([96, 23]) torch.Size([96]) torch.Size([96])\n"
          ]
        }
      ],
      "source": [
        "# Toy dataset dataloader\n",
        "train_data = ToyDataset(partition='train')\n",
        "val_data = ToyDataset(partition='valid')\n",
        "train_loader = torch.utils.data.DataLoader(train_data, num_workers=2,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle=True, collate_fn=train_data.collate_fn)\n",
        "val_loader   = torch.utils.data.DataLoader(val_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle=False, collate_fn=train_data.collate_fn)\n",
        "\n",
        "# Main dataset dataloaders\n",
        "\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0EgRqsKDI7dD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ModularListener(\n",
            "  (embedding): Conv1d(15, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (base_lstm): LSTM(128, 64)\n",
            "  (pBLSTMs): Sequential(\n",
            "    (p_BLSTM-1): pBLSTM(\n",
            "      (blstm): LSTM(64, 128, bidirectional=True)\n",
            "    )\n",
            "    (locked_dropout-1): lstm_locked_dropout()\n",
            "    (p_BLSTM-2): pBLSTM(\n",
            "      (blstm): LSTM(128, 256, bidirectional=True)\n",
            "    )\n",
            "    (locked_dropout-2): lstm_locked_dropout()\n",
            "    (p_BLSTM-3): pBLSTM(\n",
            "      (blstm): LSTM(256, 512, bidirectional=True)\n",
            "    )\n",
            "    (locked_dropout-3): lstm_locked_dropout()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "input.size(-1) must be equal to input_size. Expected 128, got 15",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/josephbajor/Dev/CMU_IDL/LAS_CMU/HW4P2_F22_Starter.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbajor/Dev/CMU_IDL/LAS_CMU/HW4P2_F22_Starter.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m encoder \u001b[39m=\u001b[39m ModularListener(hparams\u001b[39m=\u001b[39mhparams)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbajor/Dev/CMU_IDL/LAS_CMU/HW4P2_F22_Starter.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(encoder)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephbajor/Dev/CMU_IDL/LAS_CMU/HW4P2_F22_Starter.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m summary(encoder, x, xl)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josephbajor/Dev/CMU_IDL/LAS_CMU/HW4P2_F22_Starter.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdel\u001b[39;00m encoder\n",
            "File \u001b[0;32m~/mambaforge/envs/dl/lib/python3.10/site-packages/torchsummaryX/torchsummaryX.py:86\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 86\u001b[0m         model(x) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (kwargs \u001b[39mor\u001b[39;00m args) \u001b[39melse\u001b[39;00m model(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     87\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m hooks:\n",
            "File \u001b[0;32m~/mambaforge/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Dev/CMU_IDL/LAS_CMU/model.py:165\u001b[0m, in \u001b[0;36mModularListener.forward\u001b[0;34m(self, x, xl)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, xl):\n\u001b[0;32m--> 165\u001b[0m     x \u001b[39m=\u001b[39m pack_padded_sequence(x, xl, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, enforce_sorted\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    167\u001b[0m     x, (h_n, c_n) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_lstm(x)\n\u001b[1;32m    169\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpBLSTMs(x)\n",
            "File \u001b[0;32m~/mambaforge/envs/dl/lib/python3.10/site-packages/torch/nn/utils/rnn.py:262\u001b[0m, in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    258\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m batch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    259\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mindex_select(batch_dim, sorted_indices)\n\u001b[1;32m    261\u001b[0m data, batch_sizes \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 262\u001b[0m     _VF\u001b[39m.\u001b[39;49m_pack_padded_sequence(\u001b[39minput\u001b[39;49m, lengths, batch_first)\n\u001b[1;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[39mNone\u001b[39;00m)\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ],
      "source": [
        "from model import ModularListener\n",
        "\n",
        "encoder = ModularListener(hparams=hparams)\n",
        "print(encoder)\n",
        "summary(encoder, x, lx)\n",
        "del encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJCpBcEmMVcZ"
      },
      "source": [
        "## Attention (Attend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6k9R7jKMRcZ"
      },
      "source": [
        "### Different ways to compute Attention\n",
        "\n",
        "1. Dot-product attention\n",
        "    * raw_weights = bmm(key, query) \n",
        "    * Optional: Scaled dot-product by normalizing with sqrt key dimension \n",
        "    * Check \"Attention is All You Need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n",
        "\n",
        "\n",
        "2. Cosine attention\n",
        "    * raw_weights = cosine(query, key) # almost the same as dot-product xD \n",
        "\n",
        "3. Bi-linear attention\n",
        "    * W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "    * raw_weights = bmm(key @ W, query)\n",
        "\n",
        "4. Multi-layer perceptron\n",
        "    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "\n",
        "5. Multi-Head Attention\n",
        "    * Check \"Attention is All You Need\" Section 3.2.2\n",
        "    * h = Number of heads\n",
        "    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "    * W_O: d_v -> d_v\n",
        "    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n",
        "    * raw_weights = Q @ K^T\n",
        "    * masked_raw_weights = mask(raw_weights)\n",
        "    * attention = softmax(masked_raw_weights)\n",
        "    * multi_head = attention @ V\n",
        "    * multi_head = multi_head reshaped to (B, d_v)\n",
        "    * context = multi_head @ W_O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqu-MUM8TjUO"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention): \n",
        "    # Function for plotting attention\n",
        "    # You need to get a diagonal plot\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using the key, value (from encoder hidden states) and query from decoder.\n",
        "    Here are different ways to compute attention and context:\n",
        "\n",
        "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
        "\n",
        "    masked_raw_weights  = mask(raw_weights) # mask out padded elements with big negative number (e.g. -1e9 or -inf in FP16)\n",
        "    attention           = softmax(masked_raw_weights)\n",
        "    context             = bmm(attention, value)\n",
        "    \n",
        "    At the end, you can pass context through a linear layer too.\n",
        "\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, encoder_hidden_size, decoder_output_size, projection_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.key_projection     = # TODO: Define an nn.Linear layer which projects the encoder_hidden_state to keys\n",
        "        self.value_projection   = # TODO: Define an nn.Linear layer which projects the encoder_hidden_state to value\n",
        "        self.query_projection   = # TODO: Define an nn.Linear layer which projects the decoder_output_state to query\n",
        "        # Optional : Define an nn.Linear layer which projects the context vector\n",
        "\n",
        "        self.softmax            = # TODO: Define a softmax layer. Think about the dimension which you need to apply \n",
        "        # Tip: What is the shape of energy? And what are those?\n",
        "\n",
        "    # As you know, in the attention mechanism, the key, value and mask are calculated only once.\n",
        "    # This function is used to calculate them and set them to self\n",
        "    def set_key_value_mask(self, encoder_outputs, encoder_lens):\n",
        "    \n",
        "        _, encoder_max_seq_len, _ = encoder_outputs.shape\n",
        "\n",
        "        self.key      = # TODO: Project encoder_outputs using key_projection to get keys\n",
        "        self.value    = # TODO: Project encoder_outputs using value_projection to get values\n",
        "\n",
        "        # encoder_max_seq_len is of shape (batch_size, ) which consists of the lengths encoder output sequences in that batch\n",
        "        # The raw_weights are of shape (batch_size, timesteps)\n",
        "\n",
        "        # TODO: To remove the influence of padding in the raw_weights, we want to create a boolean mask of shape (batch_size, timesteps) \n",
        "        # The mask is False for all indicies before padding begins, True for all indices after.\n",
        "        self.padding_mask     =  # TODO: You want to use a comparison between encoder_max_seq_len and encoder_lens to create this mask. \n",
        "        # (Hint: Broadcasting gives you a one liner)\n",
        "        \n",
        "    def forward(self, decoder_output_embedding):\n",
        "        # key   : (batch_size, timesteps, projection_size)\n",
        "        # value : (batch_size, timesteps, projection_size)\n",
        "        # query : (batch_size, projection_size)\n",
        "\n",
        "        self.query         = # TODO: Project the query using query_projection\n",
        "\n",
        "        # Hint: Take a look at torch.bmm for the products below \n",
        "\n",
        "        raw_weights        = # TODO: Calculate raw_weights which is the product of query and key, and is of shape (batch_size, timesteps)\n",
        "        masked_raw_weights = # TODO: Mask the raw_weights with self.padding_mask. \n",
        "        # Take a look at pytorch's masked_fill_ function (You want the fill value to be a big negative number for the softmax to make it close to 0)\n",
        "\n",
        "        attention_weights  = # TODO: Calculate the attention weights, which is the softmax of raw_weights\n",
        "        context            = # TODO: Calculate the context - it is a product between attention_weights and value\n",
        "\n",
        "        # Hint: You might need to use squeeze/unsqueeze to make sure that your operations work with bmm\n",
        "\n",
        "        return context, attention_weights # Return the context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78XOdWExMSi-"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuUQTy2NMlbT"
      },
      "source": [
        "### Speller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7-R6BTuT8dm"
      },
      "outputs": [],
      "source": [
        "class Speller(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module= None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size         = vocab_size\n",
        "\n",
        "        self.embedding          = # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n",
        "\n",
        "        self.lstm_cells         = torch.nn.Sequential(\n",
        "                                # Create Two LSTM Cells as per LAS Architecture\n",
        "                                # What should the input_size of the first LSTM Cell? \n",
        "                                # Hint: It takes in a combination of the character embedding and context from attention\n",
        "                                )\n",
        "    \n",
        "                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n",
        "                                # Think why we need this in terms of the query\n",
        "\n",
        "        self.char_prob          = # TODO: Initialize the classification layer to generate your probability distribution over all characters\n",
        "\n",
        "        self.char_prob.weight   = self.embedding.weight # Weight tying\n",
        "\n",
        "        self.attention          = attention_module\n",
        "\n",
        "    \n",
        "    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1): \n",
        "\n",
        "        '''\n",
        "        Args: \n",
        "            embedding: Attention embeddings \n",
        "            hidden_list: List of Hidden States for the LSTM Cells\n",
        "        ''' \n",
        "\n",
        "        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape\n",
        "\n",
        "        if self.training:\n",
        "            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n",
        "            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n",
        "        else:\n",
        "            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n",
        "        \n",
        "\n",
        "        # INITS\n",
        "        predictions     = []\n",
        "\n",
        "        # Initialize the first character input to your decoder, SOS\n",
        "        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n",
        "\n",
        "        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n",
        "        hidden_states   = [None]*len(self.decoder.lstm_cells) \n",
        "\n",
        "        attention_plot          = []\n",
        "        context                 = # TODO: Initialize context (You have a few choices, refer to the writeup )\n",
        "        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n",
        "\n",
        "        # Set Attention Key, Value, Padding Mask just once\n",
        "        if self.attention != None:\n",
        "            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n",
        "\n",
        "\n",
        "        for t in range(timesteps):\n",
        "            \n",
        "            char_embed = #TODO: Generate the embedding for the character at timestep t\n",
        "\n",
        "            if self.training and t > 0:\n",
        "                # TODO: We want to decide which embedding to use as input for the decoder during training\n",
        "                # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n",
        "                # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n",
        "                # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n",
        "\n",
        "                char_embed = # TODO\n",
        "      \n",
        "            decoder_input_embedding = # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n",
        "            \n",
        "            # Loop over your lstm cells\n",
        "            # Each lstm cell takes in an embedding \n",
        "            for i in range(len(self.lstm_cells)):\n",
        "                # An LSTM Cell returns (h,c) -> h = hidden state, c = cell memory state\n",
        "                # Using 2 LSTM Cells is akin to a 2 layer LSTM looped through t timesteps \n",
        "                # The second LSTM Cell takes in the output hidden state of the first LSTM Cell (from the current timestep) as Input, along with the hidden and cell states of the cell from the previous timestep\n",
        "                hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n",
        "                decoder_input_embedding = hidden_states[i][0]\n",
        "\n",
        "            # The output embedding from the decoder is the hidden state of the last LSTM Cell\n",
        "            decoder_output_embedding = hidden_states[-1][0]\n",
        "\n",
        "            # We compute attention from the output of the last LSTM Cell\n",
        "            if self.attention != None:\n",
        "                context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n",
        "\n",
        "            attention_plot.append(attention_weights[0].detach().cpu())\n",
        "\n",
        "            output_embedding     = # TODO: Concatenate the projected query with context for the output embedding\n",
        "            # Hint: How can you get the projected query from attention\n",
        "            # If you are not using attention, what will you use instead of query?\n",
        "\n",
        "            char_prob            = self.char_prob(output_embedding)\n",
        "            \n",
        "            # Append the character probability distribution to the list of predictions \n",
        "            predictions.append(char_prob)\n",
        "\n",
        "            char = # TODO: Get the predicted character for the next timestep from the probability distribution \n",
        "            # (Hint: Use Greedy Decoding for starters)\n",
        "\n",
        "        attention_plot  = # TODO: Stack list of attetion_plots \n",
        "        predictions     = # TODO: Stack list of predictions \n",
        "\n",
        "        return predictions, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMgncQmVMnCO"
      },
      "source": [
        "## Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWWzurvXM0iv"
      },
      "source": [
        "### LAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcTC4cK95TYT"
      },
      "outputs": [],
      "source": [
        "class LAS(torch.nn.Module):\n",
        "    def __init__(self, input_size, encoder_hidden_size, \n",
        "                 vocab_size, embed_size,\n",
        "                 decoder_hidden_size, decoder_output_size,\n",
        "                 projection_size= 128):\n",
        "        \n",
        "        super(LAS, self).__init__()\n",
        "\n",
        "        self.encoder        = # TODO: Initialize Encoder\n",
        "        attention_module    = # TODO: Initialize Attention\n",
        "        self.decoder        = # TODO: Initialize Decoder, make sure you pass the attention module \n",
        "\n",
        "    def forward(self, x, x_lens, y = None, tf_rate = 1):\n",
        "\n",
        "        encoder_outputs, encoder_lens = self.encoder(x, x_lens) # from Listener\n",
        "        predictions, attention_plot = self.decoder(encoder_outputs, encoder_lens, y, tf_rate)\n",
        "        \n",
        "        return predictions, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHMzR6fLht5n"
      },
      "source": [
        "# Training Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI2AKhQ6YP6F"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS-YUHQlYQmL"
      },
      "outputs": [],
      "source": [
        "# Baseline LAS has the following configuration:\n",
        "# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n",
        "# Decoder Embedding Layer Dimension of 256\n",
        "# Decoder Hidden Dimension of 512 \n",
        "# Decoder Output Dimension of 128\n",
        "# Attention Projection Size of 128\n",
        "# Feel Free to Experiment with this \n",
        "\n",
        "model = LAS(\n",
        "    # Initialize your model \n",
        "    # Read the paper and think about what dimensions should be used\n",
        "    # You can experiment on these as well, but they are not requried for the early submission\n",
        "    # Remember that if you are using weight tying, some sizes need to be the same\n",
        ")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "summary(model, \n",
        "        x= example_batch[0].to(DEVICE), \n",
        "        x_lens= example_batch[3], \n",
        "        y= example_batch[1].to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDmcYul-YSdC"
      },
      "source": [
        "## Optimizer, Scheduler, Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HwmgDSvbtmd"
      },
      "outputs": [],
      "source": [
        "optimizer   = torch.optim.Adam(model.parameters(), lr= config['lr'], amsgrad= True, weight_decay= 5e-6)\n",
        "criterion   = torch.nn.CrossEntropyLoss(reduction='none') # Why are we using reduction = 'none' ? \n",
        "scaler      = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Optional: Create a custom class for a Teacher Force Schedule "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baHCja89YV-m"
      },
      "source": [
        "# Levenshtein Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDYZnnLbqJ8J"
      },
      "outputs": [],
      "source": [
        "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
        "def indices_to_chars(indices, vocab):\n",
        "    tokens = []\n",
        "    for i in indices: # This loops through all the indices\n",
        "        if vocab[int(i)] == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
        "            continue\n",
        "        elif vocab[int(i)] == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens.append(vocab[i])\n",
        "    return tokens\n",
        "\n",
        "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
        "def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n",
        "\n",
        "    dist                = 0\n",
        "    batch_size, seq_len = predictions.shape\n",
        "\n",
        "    for batch_idx in range(batch_size): \n",
        "\n",
        "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
        "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
        "\n",
        "        # Strings - When you are using characters from the AudioDataset\n",
        "        y_string    = ''.join(y_sliced)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "        \n",
        "        dist        += Levenshtein.distance(pred_string, y_string)\n",
        "        # Comment the above abd uncomment below for toy dataset \n",
        "        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
        "\n",
        "    if print_example: \n",
        "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
        "        print(\"Ground Truth : \", y_string)\n",
        "        print(\"Prediction   : \", pred_string)\n",
        "        \n",
        "    dist/=batch_size\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zjfF88iZ4Nc"
      },
      "source": [
        "# Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIXzhQclhs98"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    running_loss        = 0.0\n",
        "    running_perplexity  = 0.0\n",
        "    \n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            predictions, attention_plot = model(x, lx, y= y, tf_rate= teacher_forcing_rate)\n",
        "\n",
        "            # Predictions are of Shape (batch_size, timesteps, vocab_size). \n",
        "            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n",
        "            # So in total, you have batch_size*timesteps amount of characters.\n",
        "            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n",
        "            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n",
        "            loss        =  # TODO: Cross Entropy Loss\n",
        "\n",
        "            mask        = # TODO: Create a boolean mask using the lengths of your transcript that remove the influence of padding indices (in transcripts) in the loss \n",
        "            masked_loss = # Product between the mask and the loss, divided by the mask's sum. Hint: You may want to reshape the mask too \n",
        "            perplexity  = torch.exp(masked_loss) # Perplexity is defined the exponential of the loss\n",
        "\n",
        "            running_loss        += masked_loss.item()\n",
        "            running_perplexity  += perplexity.item()\n",
        "        \n",
        "        # Backward on the masked loss\n",
        "        scaler.scale(masked_loss).backward()\n",
        "\n",
        "        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
        "        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
        "            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    running_loss /= len(dataloader)\n",
        "    running_perplexity /= len(dataloader)\n",
        "    batch_bar.close()\n",
        "\n",
        "    return running_loss, running_perplexity, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jzpCjd9R5VYV"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "        # Greedy Decoding\n",
        "        greedy_predictions   =  # TODO: How do you get the most likely character from each distribution in the batch?\n",
        "\n",
        "        # Calculate Levenshtein Distance\n",
        "        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_lev_dist /= len(dataloader)\n",
        "\n",
        "    return running_lev_dist#, running_loss, running_perplexity, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9uRTbbPkeL-"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVVo8bAab_5N"
      },
      "outputs": [],
      "source": [
        "# Login to Wandb\n",
        "# Initialize your Wandb Run Here\n",
        "# Optional: Save your model architecture in a txt file, and save the file to Wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99rJQUdPkCUq"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s12S_bPMcguA"
      },
      "outputs": [],
      "source": [
        "best_lev_dist = float(\"inf\")\n",
        "tf_rate = 1.0\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "    \n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    # Call train and validate \n",
        "\n",
        "    # Print your metrics\n",
        "\n",
        "    # Plot Attention \n",
        "    plot_attention(attention_plot)\n",
        "\n",
        "    # Log metrics to Wandb\n",
        "\n",
        "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
        "\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        # Save your model checkpoint here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3iickk_kJNB"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7HihzA4ViiR"
      },
      "outputs": [],
      "source": [
        "# Optional: Load your best model Checkpoint here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTBD49c_kKs3"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a testing function similar to validation \n",
        "# TODO: Create a file with all predictions \n",
        "# TODO: Submit to Kaggle"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Vlev_Tvq_bRz",
        "lr4xGzRU-KZz",
        "OALQCI0EDCwh",
        "X7J4sY1OW9Pr",
        "YWRjucnUdbQ1",
        "i5ioyn6ldQB9",
        "gQenneVsDLnX",
        "lCbwz0LZMWwe",
        "XoI0zEoIMX5I",
        "_rchbyjlMeB2",
        "JJCpBcEmMVcZ",
        "f6k9R7jKMRcZ"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('idl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "47c922ff4014cf13b3a61ef7249fe62905f16a10d99c5ffef0903b4abd089351"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
